{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:09:32,949 : INFO : loading Word2VecKeyedVectors object from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model\n",
      "2020-03-31 20:09:33,327 : INFO : loading wv recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.wv.* with mmap=None\n",
      "2020-03-31 20:09:33,328 : INFO : loading vectors from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.wv.vectors.npy with mmap=None\n",
      "2020-03-31 20:09:35,334 : INFO : loading vectors_vocab from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.wv.vectors_vocab.npy with mmap=None\n",
      "2020-03-31 20:09:37,354 : INFO : loading vectors_ngrams from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.wv.vectors_ngrams.npy with mmap=None\n",
      "2020-03-31 20:09:45,833 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-03-31 20:09:45,834 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-03-31 20:09:45,835 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2020-03-31 20:09:45,835 : INFO : setting ignored attribute buckets_word to None\n",
      "2020-03-31 20:09:45,836 : INFO : loading vocabulary recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.vocabulary.* with mmap=None\n",
      "2020-03-31 20:09:45,836 : INFO : loading trainables recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.trainables.* with mmap=None\n",
      "2020-03-31 20:09:45,836 : INFO : loading syn1neg from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-03-31 20:09:45,887 : INFO : loading vectors_vocab_lockf from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.trainables.vectors_vocab_lockf.npy with mmap=None\n",
      "2020-03-31 20:09:45,937 : INFO : loading vectors_ngrams_lockf from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2020-03-31 20:09:46,526 : INFO : loaded /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "import fse # fast sentence embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path_w2v = '/home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_200d_5w_10i_5mc_sentence.model'\n",
    "\n",
    "covid_trained_model = KeyedVectors.load(path_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am using word embeddings from the CORD-19\n",
    "import glob\n",
    "import nltk\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "\n",
    "# start the log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "\n",
    "def clean_my_text(full_text_):\n",
    "    stopset = set(stopwords.words('english')) #| set(string.punctuation)\n",
    "    tokens = TreebankWordTokenizer().tokenize(full_text_.split('---10')[0])\n",
    "    cleanup = [token for token in tokens if token not in stopset and len(token) > 1]\n",
    "    return cleanup\n",
    "\n",
    "\n",
    "# get the current working directory and file\n",
    "dir_path = '/home/santosh/Work/Datasets/COVID-CORD-19_paragraphs/'#'/home/santosh/Work/Datasets/CORD-19-sentences/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4427/4427 [01:34<00:00, 46.89it/s]\n"
     ]
    }
   ],
   "source": [
    "paragraphs = []\n",
    "\n",
    "all_text_files = sorted(glob.glob(dir_path + '*.txt*'))\n",
    "\n",
    "for each_text_file in tqdm(all_text_files):\n",
    "    with open(each_text_file, 'r') as f:\n",
    "        temp_ = f.readlines()\n",
    "        for each_line in temp_:\n",
    "            paragraphs.append(TreebankWordTokenizer().tokenize(each_line))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def dump(data,filename):\n",
    "    file = open(result_dir_path+filename+'.bin','wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()\n",
    "    \n",
    "def load(filename):\n",
    "    file = open(result_dir_path+filename+'.bin','rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir_path = '/home/santosh/Work/models/word2vec/CORD-19/'\n",
    "from fse import IndexedList\n",
    "\n",
    "paragraphs_index = IndexedList(paragraphs)\n",
    "dump(paragraphs_index, 'paragraphs_index_covid-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir_path = '/home/santosh/Work/models/word2vec/CORD-19/'\n",
    "paragraphs_index = load('paragraphs_index_covid-19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:12:14,274 : INFO : no frequency mode: using wordfreq for estimation of frequency for language: en\n",
      "2020-03-31 20:12:14,912 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-03-31 20:12:15,121 : INFO : finished scanning 181169 sentences with an average length of 142 and 25750678 total words\n",
      "2020-03-31 20:12:15,149 : INFO : estimated memory for 181169 sentences with 200 dimensions and 165124 vocabulary: 1916 MB (1 GB)\n",
      "2020-03-31 20:12:15,150 : INFO : initializing sentence vectors for 181169 sentences\n",
      "2020-03-31 20:12:15,761 : INFO : pre-computing uSIF weights for 165124 words\n",
      "2020-03-31 20:12:16,111 : INFO : begin training\n",
      "2020-03-31 20:12:21,204 : INFO : PROGRESS : finished 14.00% with 25362 sentences and 3500770 words, 5072 sentences/s\n",
      "2020-03-31 20:12:26,206 : INFO : PROGRESS : finished 27.38% with 49598 sentences and 7002268 words, 4847 sentences/s\n",
      "2020-03-31 20:12:31,306 : INFO : PROGRESS : finished 41.01% with 74293 sentences and 10527974 words, 4939 sentences/s\n",
      "2020-03-31 20:12:36,311 : INFO : PROGRESS : finished 54.70% with 99100 sentences and 14126519 words, 4961 sentences/s\n",
      "2020-03-31 20:12:41,314 : INFO : PROGRESS : finished 68.63% with 124337 sentences and 17640939 words, 5047 sentences/s\n",
      "2020-03-31 20:12:46,325 : INFO : PROGRESS : finished 81.73% with 148077 sentences and 21063403 words, 4748 sentences/s\n",
      "2020-03-31 20:12:51,340 : INFO : PROGRESS : finished 95.51% with 173029 sentences and 24658594 words, 4990 sentences/s\n",
      "2020-03-31 20:12:52,812 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-03-31 20:12:52,839 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-03-31 20:12:52,839 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-31 20:12:52,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-31 20:12:52,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-31 20:12:52,843 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-31 20:12:53,894 : INFO : computing 5 principal components took 1s\n",
      "2020-03-31 20:12:54,105 : INFO : removing 5 principal components took 0s\n",
      "2020-03-31 20:12:54,107 : INFO : training on 181169 effective sentences with 25750678 effective words took 36s with 4932 sentences/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(181169, 25750678)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SIF embeddings\n",
    "from fse.models import uSIF\n",
    "sif_model = uSIF(covid_trained_model, workers=6, lang_freq=\"en\")\n",
    "\n",
    "sif_model.train(paragraphs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:13:20,934 : INFO : saving uSIF object under /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model, separately None\n",
      "2020-03-31 20:13:20,935 : INFO : storing np array 'vectors' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model.wv.vectors.npy\n",
      "2020-03-31 20:13:21,003 : INFO : storing np array 'vectors_vocab' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model.wv.vectors_vocab.npy\n",
      "2020-03-31 20:13:21,066 : INFO : storing np array 'vectors_ngrams' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model.wv.vectors_ngrams.npy\n",
      "2020-03-31 20:13:56,493 : INFO : storing np array 'vectors' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model.sv.vectors.npy\n",
      "2020-03-31 20:14:01,389 : INFO : saved /home/santosh/Work/models/word2vec/CORD-19/CORD-19-FT_covid_papers_p2v_sif.model\n"
     ]
    }
   ],
   "source": [
    "sif_model.save(result_dir_path+'CORD-19-FT_covid_papers_p2v_sif.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:48:57,917 : INFO : loading Word2VecKeyedVectors object from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model\n",
      "2020-03-31 20:48:59,373 : INFO : loading wv recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model.wv.* with mmap=None\n",
      "2020-03-31 20:48:59,374 : INFO : loading vectors from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model.wv.vectors.npy with mmap=None\n",
      "2020-03-31 20:49:02,913 : INFO : loading sv recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model.sv.* with mmap=None\n",
      "2020-03-31 20:49:02,915 : INFO : loading vectors from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model.sv.vectors.npy with mmap=None\n",
      "2020-03-31 20:49:37,617 : INFO : loading prep recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model.prep.* with mmap=None\n",
      "2020-03-31 20:49:37,618 : INFO : loaded /home/santosh/Work/models/word2vec/CORD-19/CORD-19_sif_sv.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "sif_model = KeyedVectors.load(result_dir_path+'CORD-19_sif_sv.model')# 'CORD-19-FT_covid_papers_p2v_sif.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_query_result(sv_query_result):\n",
    "    result_sentences =[]\n",
    "    for each_result in sv_query_result:\n",
    "        result_sentences.append(TreebankWordDetokenizer().detokenize((each_result[0])))\n",
    "     \n",
    "    return result_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==2.2.0\n",
    "# !pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Summarizer()\n",
    "# body = ' '.join(extract_query_result(query_result))\n",
    "# result = model(body, min_length=200)\n",
    "# full = ''.join(result)\n",
    "# full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def convert_html(list_sentences):\n",
    "    html_markup_list = []\n",
    "    for each_result in list_sentences:\n",
    "        sentence = '<p> '+each_result.split('---10')[0]+'</p>'\n",
    "        source = '<a href = \"https://doi.org/'+each_result.split('---')[1].replace('---','').strip()+'\">[source]</a>'\n",
    "        \n",
    "        html_markup_list.append(sentence+' '+source)\n",
    "        \n",
    "    return html_markup_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:49:45,039 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-03-31 20:49:45,041 : INFO : finished scanning 1 sentences with an average length of 4 and 4 total words\n",
      "2020-03-31 20:49:45,044 : INFO : removing 5 principal components took 0s\n",
      "2020-03-31 20:49:45,045 : INFO : precomputing L2-norms of sentence vectors\n"
     ]
    }
   ],
   "source": [
    "# query = \"pre-existing pulmonary disease SARS-Cov2 Hypertension\" \n",
    "\n",
    "# query = \"What is the incubation days of SARS-CoV-2\" \n",
    "# query = \"incubation days coronavirus 2019-nCoV\"#  COVID-19\n",
    "# query = 'socio economic poverty behaviour'\n",
    "\n",
    "# query = 'what is the comorbidities associated with death'\n",
    "# query = 'public health mitigation measures that could be effective for control'\n",
    "# query = 'socio-economic and behavioral factors to understand the economic impact of the SARS-CoV-2 virus and whether there were differences. '\n",
    "# query = 'what are the risk factors for death in COVID-19'\n",
    "# query = 'what is the basic reproductive number of SARS-CoV-2 in days'\n",
    "# query = 'what is the serial interval days SARS-CoV-2'\n",
    "# query = 'what do we know about the environmental factors influencing SARS-CoV-2'\n",
    "# query = 'what do we know about medication COVID-19'\n",
    "query = 'Transmission dynamics of the virus SARS-CoV-2'\n",
    "# query ='risk of fatality among symptomatic hospitalized patients in SARS-CoV-2'\n",
    "# query = 'Efforts targeted at a universal coronavirus vaccine'\n",
    "\n",
    "\n",
    "query_result = sif_model.sv.similar_by_sentence(clean_my_text(query), model=sif_model, indexable=paragraphs_index.items, topn=10)\n",
    "\n",
    "extract_query_result(query_result)\n",
    "# summarize(' '.join(extract_query_result(query_result)), ratio =0.2, split=True)\n",
    "\n",
    "display(HTML('<hr>\\n'.join(convert_html(extract_query_result(query_result)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scispacy]",
   "language": "python",
   "name": "conda-env-scispacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
