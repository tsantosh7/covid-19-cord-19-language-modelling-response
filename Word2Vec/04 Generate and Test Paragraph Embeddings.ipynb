{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "import fse # fast sentence embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path_w2v = '/home/santosh/Work/models/word2vec/CORD-19/CORD-10-w2v_200d_5w_10i_3mc.bin'\n",
    "\n",
    "covid_trained_model = KeyedVectors.load_word2vec_format(path_w2v, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am using word embeddings from the CORD-19\n",
    "import glob\n",
    "import nltk\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# start the log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "# get the current working directory and file\n",
    "result_dir_path = '/home/santosh/Work/Datasets/CORD-19-paragraphs/'\n",
    "\n",
    "root_path = '/home/santosh/Work/Datasets/CORD-19-research-challenge/'\n",
    "paths = ['biorxiv_medrxiv/biorxiv_medrxiv/',\n",
    "        'comm_use_subset/comm_use_subset/',\n",
    "        'noncomm_use_subset/noncomm_use_subset/']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the jsosn\n",
    "import json\n",
    "\n",
    "all_jsons =[]\n",
    "for json_path in paths:\n",
    "     all_jsons.extend(sorted(glob.glob(root_path+json_path + '*.json*')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12356/12356 [00:59<00:00, 209.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract sentences\n",
    "\n",
    "for each_json_file in tqdm(all_jsons):\n",
    "    with open(each_json_file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "        full_text = []\n",
    "\n",
    "        try:\n",
    "            for each_text in data['abstract']:\n",
    "                full_text.append(each_text['text'])\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            for each_text in data['body_text']:\n",
    "                full_text.append(each_text['text'])\n",
    "        except:\n",
    "                pass\n",
    "        \n",
    "        with open(result_dir_path+each_json_file.split('/')[-1][:-5]+'.txt', 'a') as writer:\n",
    "            for each_para in full_text:\n",
    "                    if 'word count' not in each_para and 'All rights reserved' not in each_para and 'No reuse allowed without permission' not in each_para:\n",
    "                        writer.write(each_para+'\\n')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12356/12356 [07:11<00:00, 28.60it/s]\n"
     ]
    }
   ],
   "source": [
    "paragraphs = []\n",
    "\n",
    "all_text_files = sorted(glob.glob(result_dir_path + '*.txt*'))\n",
    "\n",
    "for each_text_file in tqdm(all_text_files):\n",
    "    with open(each_text_file, 'r') as f:\n",
    "        temp_ = f.readlines()\n",
    "        for each_line in temp_:\n",
    "            paragraphs.append(nltk.word_tokenize(each_line))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fse import IndexedList\n",
    "paragraphs_index = IndexedList(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def dump(data,filename):\n",
    "    file = open(result_dir_path+filename+'.bin','wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir_path = '/home/santosh/Work/models/word2vec/CORD-19/'\n",
    "dump(paragraphs_index, 'paragraphs_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 21:48:05,006 : INFO : no frequency mode: using wordfreq for estimation of frequency for language: en\n",
      "2020-03-29 21:48:05,165 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-03-29 21:48:05,628 : INFO : finished scanning 413898 sentences with an average length of 158 and 65397441 total words\n",
      "2020-03-29 21:48:05,676 : INFO : estimated memory for 413898 sentences with 200 dimensions and 241336 vocabulary: 500 MB (0 GB)\n",
      "2020-03-29 21:48:05,677 : INFO : initializing sentence vectors for 413898 sentences\n",
      "2020-03-29 21:48:07,008 : INFO : pre-computing uSIF weights for 241336 words\n",
      "2020-03-29 21:48:07,524 : INFO : begin training\n",
      "2020-03-29 21:48:12,530 : INFO : PROGRESS : finished 42.06% with 174083 sentences and 13559343 words, 34816 sentences/s\n",
      "2020-03-29 21:48:17,533 : INFO : PROGRESS : finished 83.86% with 347097 sentences and 27192688 words, 34602 sentences/s\n",
      "2020-03-29 21:48:19,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-03-29 21:48:19,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-29 21:48:19,586 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-29 21:48:19,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-29 21:48:22,634 : INFO : computing 5 principal components took 3s\n",
      "2020-03-29 21:48:23,018 : INFO : removing 5 principal components took 0s\n",
      "2020-03-29 21:48:23,019 : INFO : training on 413898 effective sentences with 32531620 effective words took 12s with 34310 sentences/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(413898, 32531620)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SIF embeddings\n",
    "from fse.models import uSIF\n",
    "sif_model = uSIF(covid_trained_model, workers=4, lang_freq=\"en\")\n",
    "\n",
    "sif_model.train(paragraphs_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-29 21:48:53,520 : INFO : saving uSIF object under /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin, separately None\n",
      "2020-03-29 21:48:53,523 : INFO : storing np array 'vectors' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.wv.vectors.npy\n",
      "2020-03-29 21:48:53,626 : INFO : storing np array 'vectors' to /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.sv.vectors.npy\n",
      "2020-03-29 21:48:54,216 : INFO : saved /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin\n"
     ]
    }
   ],
   "source": [
    "sif_model.save(result_dir_path+'CORD-19-p2v_sif.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def load(filename):\n",
    "    file = open(path_w2v+filename+'.bin','rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_w2v = '/home/santosh/Work/models/word2vec/CORD-19/'\n",
    "\n",
    "paragraphs_index = load('paragraphs_index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-30 09:27:00,341 : INFO : loading Word2VecKeyedVectors object from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin\n",
      "2020-03-30 09:27:03,011 : INFO : loading wv recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.wv.* with mmap=None\n",
      "2020-03-30 09:27:03,012 : INFO : loading vectors from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.wv.vectors.npy with mmap=None\n",
      "2020-03-30 09:27:05,548 : INFO : loading sv recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.sv.* with mmap=None\n",
      "2020-03-30 09:27:05,550 : INFO : loading vectors from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.sv.vectors.npy with mmap=None\n",
      "2020-03-30 09:27:09,928 : INFO : loading prep recursively from /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin.prep.* with mmap=None\n",
      "2020-03-30 09:27:09,930 : INFO : loaded /home/santosh/Work/models/word2vec/CORD-19/CORD-19-p2v_sif.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "sif_model = KeyedVectors.load(path_w2v+'CORD-19-p2v_sif.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    step7 = step6.replace(\"[ \", \"[\").replace(\" ]\", \"]\")\n",
    "    return step7.strip()\n",
    "\n",
    "def extract_query_result(sv_query_result):\n",
    "    result_sentences =[]\n",
    "    for each_result in sv_query_result:\n",
    "        result_sentences.append(untokenize(each_result[0]))\n",
    "     \n",
    "    return result_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-30 17:38:11,432 : INFO : scanning all indexed sentences and their word counts\n",
      "2020-03-30 17:38:11,432 : INFO : finished scanning 1 sentences with an average length of 7 and 7 total words\n",
      "2020-03-30 17:38:11,434 : INFO : removing 5 principal components took 0s\n",
      "2020-03-30 17:38:11,487 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-30 17:38:11,491 : INFO : built Dictionary(297 unique tokens: ['complic', 'evolut', 'futur', 'influenza', 'issu']...) from 48 documents (total 655 corpus positions)\n",
      "2020-03-30 17:38:11,494 : INFO : Building graph\n",
      "2020-03-30 17:38:11,495 : INFO : Filling graph\n",
      "2020-03-30 17:38:11,515 : INFO : Removing unreachable nodes of graph\n",
      "2020-03-30 17:38:11,516 : INFO : Pagerank graph\n",
      "2020-03-30 17:38:11,529 : INFO : Sorting pagerank scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In regard to choosing a vaccine target and platform, the vaccine candidate must be immunogenic and immune targeting must lead to virus neutralization or potent cytotoxic responses.',\n",
       " 'Much of the focus for the development of a SARS-CoV or MERS-CoV vaccine has been on the S protein since it is immunogenic and antibodies targeting it can neutralize the virus [59, 60].',\n",
       " 'Although the highly conservative M2e of influenza A virus is one of the most promising target for development of universal influenza vaccines, some strategies would be required to improve immunogenicity of vaccines based on M2e containing only 24 amino acid.',\n",
       " 'In this review, we discuss promising novel influenza virus vaccine targets and the use of MVA for vaccine development against various respiratory viruses.',\n",
       " 'Early vaccine studies focused on leveraging strategies that had been successful for other vaccines including virus inactivation [12] [13] [14] and subunit immunogens [15] along with novel strategies such as recombinant viral constructs [11].',\n",
       " 'These principles can potentially be extended to many other viral subunit vaccines, such as vaccines against influenza and Ebola viruses.',\n",
       " 'This study has also produced a safe and highly effective ZIKV subunit vaccine that holds promise to be developed into a successful vaccine to protect pregnant women and their fetuses.',\n",
       " 'Since S protein and its fragments, such as RBD, of SARS-CoV, and MERS-CoV are prime targets for developing subunit vaccines against these two highly pathogenic human CoVs, it is expected that similar regions of 2019-nCoV can also be used as key targets for developing vaccines against this new coronavirus (Jiang et al., 2020).',\n",
       " 'Taken together, the approaches and strategies in the development of subunit vaccines against SARS and MERS described in this review will provide important information for the rapid design and development of safe and effective subunit vaccines against 2019-nCoV infection.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"pre-existing pulmonary disease SARS-Cov2 Hypertension\" \n",
    "\n",
    "# query = \"What is the incubation days of SARS-CoV-2\" \n",
    "# query = \"incubation days coronavirus 2019-nCoV\"#  COVID-19\n",
    "# query = 'socio economic poverty behaviour'\n",
    "\n",
    "# query = 'what is the influence of pre-existing diseases and comorbidities'\n",
    "# query = 'public health mitigation measures that could be effective for control'\n",
    "# query = 'socio-economic and behavioral factors to understand the economic impact of the SARS-CoV-2 virus and whether there were differences. '\n",
    "# query = 'what are the risk factors for death in COVID-19'\n",
    "# query = 'what is the basic reproductive number of SARS-CoV-2 in days'\n",
    "# query = 'what is the serial interval days SARS-CoV-2'\n",
    "# query = 'what do we know about the environmental factors influencing SARS-CoV-2'\n",
    "# query = 'what do we know about drugs using to treat SARS-CoV-2'\n",
    "# query = 'Transmission dynamics of the virus SARS-CoV-2'\n",
    "# query ='risk of fatality among symptomatic hospitalized patients'\n",
    "# query = 'Efforts targeted at a universal coronavirus vaccine'\n",
    "query = ''\n",
    "\n",
    "query_result = sif_model.sv.similar_by_sentence(nltk.word_tokenize(query), model=sif_model, indexable=paragraphs_index.items, topn=10)\n",
    "\n",
    "extract_query_result(query_result)\n",
    "summarize(' '.join(extract_query_result(query_result)), ratio =0.2, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scispacy]",
   "language": "python",
   "name": "conda-env-scispacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
